{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Bahrami Parameter Sweep with REM Engine\n",
    "\n",
    "This notebook demonstrates the Bahrami parameter sweep experiment using the REM (Retrieving Effectively from Memory) model.\n",
    "\n",
    "**Experiment Design:**\n",
    "- Fixed expert agent (A) with c = 0.7\n",
    "- Sweep novice agent (B) ability from c = 0.1 to 0.9\n",
    "- Test 5 group decision rules: CF, UW, DMC, DSS, BF\n",
    "- Measure Collective Benefit Ratio = d'_team / max(d'_A, d'_B)\n",
    "\n",
    "**Key Question:** Do groups perform better than their best individual member?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import run_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bahrami parameter sweep\n",
    "df = run_simulation.run_bahrami_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Part 1: Bahrami Parameter Sweep (Tim's Analysis)\n",
    "\n",
    "Compare all 5 group decision rules across varying ability heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "results = pd.read_csv('../outputs/bahrami_sweep_final.csv')\n",
    "print(\"Results Summary:\")\n",
    "print(f\"Total rows: {len(results)}\")\n",
    "print(f\"\\nColumns: {list(results.columns)}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Bahrami plot (5 rules)\n",
    "display(Image('../outputs/bahrami_sweep_plot.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Theoretical Verification (Rich's Request)\n",
    "\n",
    "Verify that the DSS (Direct Signal Sharing) rule matches the theoretical optimal sensitivity under SDT assumptions with independent noise.\n",
    "\n",
    "**Theoretical Prediction (Orthogonal Sum):**\n",
    "\n",
    "$$d'_{\\text{theory}} = \\sqrt{d'_A^2 + d'_B^2}$$\n",
    "\n",
    "This is the analytical upper bound for optimal integration of independent evidence sources.\n",
    "\n",
    "**Goal:** Check if DSS (simulated via REM) recovers this SDT prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Rich's verification plot\n",
    "display(Image('../outputs/rich_theory_verification.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison: DSS vs Theory\n",
    "dss_data = results[results['rule'] == 'DSS'].sort_values('c_B')\n",
    "print(\"DSS vs Theoretical Prediction:\")\n",
    "print(\"=\"*60)\n",
    "comparison = pd.DataFrame({\n",
    "    'c_B': dss_data['c_B'],\n",
    "    'DSS_simulated': dss_data['collective_benefit_ratio'],\n",
    "    'Theory': dss_data['ratio_theory'],\n",
    "    'Difference': dss_data['collective_benefit_ratio'] - dss_data['ratio_theory']\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nMean Absolute Difference: {:.4f}\".format(comparison['Difference'].abs().mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Confidence Miscalibration (Prelec Weighting)\n",
    "\n",
    "This section explores how **confidence miscalibration** affects group decision-making using Prelec probability weighting.\n",
    "\n",
    "### Experimental Design\n",
    "\n",
    "**Fixed Parameters:**\n",
    "- Equal memory ability: c_A = c_B = 0.7\n",
    "- Agent A miscalibration: α_A = 1.2 (overconfident)\n",
    "\n",
    "**Sweep Parameter:**\n",
    "- Agent B miscalibration: α_B from 0.5 to 1.5 (step 0.1)\n",
    "\n",
    "**Models Tested:**\n",
    "1. **WCS_Miscal**: Weighted Confidence Sharing with Prelec weighting\n",
    "2. **DMC_Miscal**: Defer to Max Confidence with Prelec weighting\n",
    "3. **DSS**: Direct Signal Sharing (α-independent benchmark)\n",
    "4. **CF**: Coin Flip (α-independent benchmark)\n",
    "\n",
    "### Prelec Weighting Function\n",
    "\n",
    "The Prelec function transforms objective probabilities into subjective weights:\n",
    "\n",
    "$$w(p) = \\exp(-\\beta \\cdot (-\\ln p)^\\alpha)$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$: Miscalibration parameter\n",
    "  - $\\alpha = 1$: Calibrated (w = p)\n",
    "  - $\\alpha > 1$: Overconfident (inflated extremes)\n",
    "  - $\\alpha < 1$: Underconfident (compressed extremes)\n",
    "- $\\beta = (\\ln 2)^{1-\\alpha}$: Ensures w(0.5) = 0.5\n",
    "\n",
    "### Key Question\n",
    "\n",
    "**How does matching vs mismatching miscalibration affect group performance?**\n",
    "- When both agents are overconfident (α_A = α_B = 1.2), do they perform better/worse?\n",
    "- When agents have opposite biases (α_A = 1.2, α_B = 0.8), does this help or hurt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run miscalibration parameter sweep\n",
    "df_miscal = run_simulation.run_miscalibration_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "results_miscal = pd.read_csv('../outputs/miscalibration_sweep.csv')\n",
    "print(\"Miscalibration Sweep Results Summary:\")\n",
    "print(f\"Total rows: {len(results_miscal)}\")\n",
    "print(f\"\\nColumns: {list(results_miscal.columns)}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "results_miscal.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the miscalibration plot\n",
    "display(Image('../outputs/miscalibration_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative analysis: Examine performance at key alpha_B values\n",
    "print(\"Performance at Key Miscalibration Levels:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "key_alphas = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "for alpha_B in key_alphas:\n",
    "    subset = results_miscal[np.isclose(results_miscal['alpha_B'], alpha_B)]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"\\nα_B = {alpha_B:.1f}:\")\n",
    "        for model in ['WCS_Miscal', 'DMC_Miscal', 'DSS', 'CF']:\n",
    "            row = subset[subset['model'] == model]\n",
    "            if len(row) > 0:\n",
    "                ratio = row['collective_benefit_ratio'].values[0]\n",
    "                print(f\"  {model:12s}: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Rich's Conflict Resolution Model\n",
    "\n",
    "This section implements Rich Shiffrin's model for how groups resolve disagreements based on evidence strength.\n",
    "\n",
    "### Core Question\n",
    "\n",
    "When two agents disagree (one says \"Old\", one says \"New\"), how does the group decide who to follow?\n",
    "\n",
    "### Model Logic\n",
    "\n",
    "1. **Convert log-odds to Strength:**\n",
    "   - Odds: $\\phi = \\exp(L)$\n",
    "   - Scaled Odds: $\\phi' = \\phi^{1/11}$ (fixed power)\n",
    "   - Strength: $S = \\max(\\phi', 1/\\phi')$\n",
    "\n",
    "2. **Identify Conflict Trials:**\n",
    "   - Conflict occurs when $(L_A > 0 \\text{ and } L_B < 0)$ OR $(L_A < 0 \\text{ and } L_B > 0)$\n",
    "\n",
    "3. **Compute Difference:**\n",
    "   - $D = |S_A - S_B|$\n",
    "\n",
    "4. **Decision Rule:**\n",
    "   - Probability of choosing stronger agent: $P = \\frac{1 + D}{2 + D}$\n",
    "   - When $D = 0$: Random guess (P = 0.5)\n",
    "   - As $D \\to \\infty$: Deterministic choice (P → 1)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Verify that the empirical probability curve follows the theoretical prediction $P = \\frac{1 + D}{2 + D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Rich's conflict resolution simulation\n",
    "df_rich = run_simulation.run_rich_conflict_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the conflict resolution plot\n",
    "display(Image('../outputs/rich_conflict_plot.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
